{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definição dos Caminhos e Nomes de Colunas ---\n",
    "\n",
    "# Caminho para a pasta onde o arquivo está localizado\n",
    "caminho_pasta = '../data/raw'\n",
    "\n",
    "# Nome do arquivo CSV\n",
    "nome_arquivo = 'Cópia de Histeroscopia Diagnóstica (respostas) 2023 - Respostas ao formulário 1.csv'\n",
    "\n",
    "# Construção do caminho completo do arquivo\n",
    "caminho_completo = os.path.join(caminho_pasta, nome_arquivo)\n",
    "\n",
    "# Lista com o nome exato das colunas que devem ser carregadas\n",
    "colunas_para_carregar = [\n",
    "    \"Idade\",\n",
    "    \"Indique sua condição de saúde atual\",\n",
    "    \"Conte um pouco sobre o motivo para solicitar a consulta. \",\n",
    "    \"Número de prontuário no CISAM\\nVocê só poderá continuar este preenchimento se tiver prontuário, caso vc não tenha prontuário no CISAM, clique aqui.\"\n",
    "]\n",
    "\n",
    "# --- 2. Carregamento do Arquivo CSV ---\n",
    "\n",
    "try:\n",
    "    # Carrega o arquivo CSV especificando apenas as colunas desejadas\n",
    "    df = pd.read_csv(caminho_completo, usecols=colunas_para_carregar)\n",
    "\n",
    "    # --- NOVA SEÇÃO: 3. Renomeando as Colunas ---\n",
    "\n",
    "    # Dicionário para mapear os nomes antigos para os novos\n",
    "    # 'nome antigo': 'nome novo'\n",
    "    novos_nomes = {\n",
    "        \"Idade\": \"idade\",\n",
    "        \"Indique sua condição de saúde atual\": \"condicao_saude\",\n",
    "        \"Conte um pouco sobre o motivo para solicitar a consulta. \": \"motivo_consulta\",\n",
    "        \"Número de prontuário no CISAM\\nVocê só poderá continuar este preenchimento se tiver prontuário, caso vc não tenha prontuário no CISAM, clique aqui.\": \"prontuario_cisam\"\n",
    "    }\n",
    "\n",
    "    # A função rename é chamada no dataframe 'df'.\n",
    "    # O parâmetro 'columns' recebe o dicionário com os novos nomes.\n",
    "    # 'inplace=True' modifica o dataframe diretamente, sem precisar criar um novo.\n",
    "    df.rename(columns=novos_nomes, inplace=True)\n",
    "\n",
    "    # --- 4. Exibição dos Dados com as Colunas Renomeadas ---\n",
    "\n",
    "    print(\"Dados carregados e colunas renomeadas com sucesso!\")\n",
    "    print(\"Visualizando as 5 primeiras linhas com os novos nomes de colunas:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nInformações sobre o DataFrame (note os novos nomes nas colunas):\")\n",
    "    df.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: O arquivo não foi encontrado no caminho especificado:\\n{caminho_completo}\")\n",
    "    print(\"\\nPor favor, verifique se o nome do arquivo e o caminho para a pasta '../data/raw' estão corretos.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Erro ao carregar as colunas. Verifique se os nomes das colunas em 'colunas_para_carregar' estão exatamente como no arquivo CSV.\")\n",
    "    print(f\"Detalhe do erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Garante que o DataFrame 'df' existe antes de iniciar ---\n",
    "if 'df' in locals():\n",
    "\n",
    "    # --- 0. Ponto de Partida: Análise inicial ---\n",
    "    print(\"--- ANÁLISE INICIAL DO DATAFRAME 'df' ---\")\n",
    "    print(f\"Número total de linhas (registros) antes da operação: {len(df)}\")\n",
    "    print(f\"Número de prontuários únicos: {df['prontuario_cisam'].nunique()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # --- 1. Remover todos os dados nulos da coluna 'prontuario_cisam' ---\n",
    "    print(\"\\n--- ETAPA 1: Removendo registros com prontuário nulo/vazio ---\")\n",
    "    df_clean = df.dropna(subset=['prontuario_cisam']).copy()\n",
    "\n",
    "    # Garantia extra: remove também strings que contêm apenas espaços em branco\n",
    "    # .astype(str) previne erros caso a coluna tenha tipos mistos\n",
    "    df_clean = df_clean[df_clean['prontuario_cisam'].astype(str).str.strip() != '']\n",
    "    print(f\"Número de linhas após remover nulos: {len(df_clean)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # --- 2. Unir valores repetidos com regras específicas ---\n",
    "    print(\"\\n--- ETAPA 2: Consolidando prontuários duplicados ---\")\n",
    "    print(\"Agrupando por 'prontuario_cisam' e aplicando regras de união...\")\n",
    "\n",
    "    # Define as regras de agregação para cada coluna\n",
    "    regras_de_agregacao = {\n",
    "        'idade': 'first',  # Pega o primeiro valor encontrado para a idade\n",
    "        'condicao_saude': lambda s: ' | '.join(s.dropna().unique()), # Concatena valores únicos com ' | '\n",
    "        'motivo_consulta': lambda s: ' | '.join(s.dropna().unique()) # Concatena valores únicos com ' | '\n",
    "    }\n",
    "\n",
    "    # Agrupa pelo prontuário e aplica as regras definidas\n",
    "    # O .reset_index() transforma a coluna de agrupamento ('prontuario_cisam') de volta em uma coluna normal\n",
    "    df_consolidado = df_clean.groupby('prontuario_cisam').agg(regras_de_agregacao).reset_index()\n",
    "\n",
    "    print(f\"Número de linhas após consolidar duplicados: {len(df_consolidado)}\")\n",
    "    print(\"Este número deve ser igual ao de prontuários únicos da análise inicial.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # --- 3. Verificar se sobraram valores repetidos ---\n",
    "    print(\"\\n--- ETAPA 3: Verificação final de duplicados ---\")\n",
    "\n",
    "    duplicados_restantes = df_consolidado['prontuario_cisam'].duplicated().any()\n",
    "\n",
    "    if not duplicados_restantes:\n",
    "        print(\"✅ SUCESSO! Não há mais nenhum valor duplicado na coluna 'prontuario_cisam'.\")\n",
    "    else:\n",
    "        print(\"❌ ATENÇÃO! A operação falhou e ainda existem prontuários duplicados.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # --- 4. Exibição do Resultado Final ---\n",
    "    print(\"\\n--- RESULTADO FINAL ---\")\n",
    "    print(\"Visualizando as 5 primeiras linhas do DataFrame consolidado e limpo:\")\n",
    "    display(df_consolidado.head())\n",
    "\n",
    "    # Verificando um exemplo de prontuário que pode ter sido concatenado (se houver duplicados)\n",
    "    # print(\"\\nExemplo de um registro que pode ter sido consolidado:\")\n",
    "    # display(df_consolidado[df_consolidado['motivo_consulta'].str.contains('\\|', na=False)])\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Erro: O DataFrame 'df' não foi encontrado. Por favor, execute a célula que o carrega primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definição do Caminho e Colunas ---\n",
    "\n",
    "# Nome do arquivo CSV\n",
    "nome_arquivo_pacientes = 'Dados pacientes - Página1.csv'\n",
    "\n",
    "# Construção do caminho completo para o arquivo\n",
    "caminho_completo_pacientes = os.path.join(caminho_pasta, nome_arquivo_pacientes)\n",
    "\n",
    "# --- Lista com as colunas específicas para carregar ---\n",
    "colunas_pacientes_para_carregar = [\n",
    "    \"Prontuário\",\n",
    "    \"Evolução\"\n",
    "]\n",
    "\n",
    "# Dicionário para especificar que a coluna 'Prontuário' deve ser lida como texto (string)\n",
    "tipos_dados = {\n",
    "    'Prontuário': str\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. Carregamento do Arquivo CSV ---\n",
    "\n",
    "try:\n",
    "    # MODIFICADO: Adicionado o parâmetro 'dtype' para forçar o tipo da coluna\n",
    "    df_pacientes = pd.read_csv(\n",
    "        caminho_completo_pacientes, \n",
    "        usecols=colunas_pacientes_para_carregar,\n",
    "        dtype=tipos_dados\n",
    "    )\n",
    "\n",
    "    # --- 3. Exibição dos Dados Carregados ---\n",
    "\n",
    "    print(\"Arquivo de pacientes carregado com sucesso (apenas colunas selecionadas)!\")\n",
    "    print(\"Visualizando as 5 primeiras linhas:\")\n",
    "    display(df_pacientes.head())\n",
    "\n",
    "    print(\"\\nInformações sobre o DataFrame de pacientes (note as colunas carregadas):\")\n",
    "    # Agora, a saída de .info() mostrará 'Prontuário' como 'object' ou 'string', que é o tipo para texto no pandas.\n",
    "    df_pacientes.info()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: O arquivo não foi encontrado no caminho especificado:\\n{caminho_completo_pacientes}\")\n",
    "    print(\"\\nPor favor, verifique se o nome do arquivo está correto e se ele se encontra na pasta '../data/raw'.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Erro ao carregar as colunas. Verifique se os nomes 'Prontuário' e 'Evolução' estão exatamente como no arquivo CSV.\")\n",
    "    print(f\"Detalhe do erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado ao tentar carregar o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Garante que o DataFrame 'df_pacientes' existe antes de iniciar ---\n",
    "if 'df_pacientes' in locals():\n",
    "\n",
    "    print(\"--- Operando no DataFrame 'df_pacientes' ---\")\n",
    "    print(f\"Número de linhas ANTES da limpeza: {len(df_pacientes)}\")\n",
    "\n",
    "    # --- 1. Remover valores vazios na coluna 'Prontuário' ---\n",
    "\n",
    "    # Criamos uma cópia para não alterar o DataFrame original\n",
    "    df_pacientes_limpo = df_pacientes.copy()\n",
    "\n",
    "    # Etapa 1.1: Remove valores nulos (NaN)\n",
    "    df_pacientes_limpo.dropna(subset=['Prontuário'], inplace=True)\n",
    "\n",
    "    # Etapa 1.2: Como a coluna é String, removemos também strings vazias ou que contêm apenas espaços\n",
    "    # O .astype(str) garante que o método .str funcione sem erros\n",
    "    # O .str.strip() remove espaços em branco do início e do fim\n",
    "    df_pacientes_limpo = df_pacientes_limpo[df_pacientes_limpo['Prontuário'].astype(str).str.strip() != '']\n",
    "\n",
    "    print(f\"Número de linhas APÓS a limpeza: {len(df_pacientes_limpo)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "    # --- 2. Verificar se existem valores duplicados após a remoção ---\n",
    "    print(\"\\n--- Verificando duplicados na coluna 'Prontuário' limpa ---\")\n",
    "\n",
    "    # O método .value_counts() conta quantas vezes cada valor aparece\n",
    "    contagem_prontuarios = df_pacientes_limpo['Prontuário'].value_counts()\n",
    "\n",
    "    # Filtramos a contagem para encontrar apenas os prontuários que aparecem mais de uma vez\n",
    "    prontuarios_duplicados = contagem_prontuarios[contagem_prontuarios > 1]\n",
    "\n",
    "    # Verifica se a série de duplicados está vazia ou não\n",
    "    if prontuarios_duplicados.empty:\n",
    "        print(\"✅ SUCESSO! Nenhum valor duplicado foi encontrado na coluna 'Prontuário' após a limpeza.\")\n",
    "    else:\n",
    "        print(f\"❌ ATENÇÃO! Foram encontrados {len(prontuarios_duplicados)} prontuários duplicados após a limpeza.\")\n",
    "        print(\"Abaixo estão os prontuários e a quantidade de vezes que eles aparecem:\")\n",
    "        # A função display formata a saída de forma mais elegante no notebook\n",
    "        display(prontuarios_duplicados)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "else:\n",
    "    print(\"Erro: O DataFrame 'df_pacientes' não foi encontrado. Por favor, execute a célula que o carrega primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Verificação dos DataFrames (Opcional, mas recomendado) ---\n",
    "# Garante que os DataFrames df_consolidado e df_pacientes_limpo existem antes de prosseguir.\n",
    "if 'df_consolidado' in locals() and 'df_pacientes_limpo' in locals():\n",
    "\n",
    "    # --- 2. Junção dos DataFrames (Merge) ---\n",
    "    # Usamos a função pd.merge para unir os dois DataFrames.\n",
    "    # how='left': Mantém todas as linhas de 'df_consolidado' (o DataFrame da esquerda).\n",
    "    # left_on='prontuario_cisam': Coluna chave do DataFrame da esquerda.\n",
    "    # right_on='Prontuário': Coluna chave do DataFrame da direita.\n",
    "    df_completo = pd.merge(df_consolidado, df_pacientes_limpo, how='inner', left_on='prontuario_cisam', right_on='Prontuário')\n",
    "\n",
    "    # --- 3. Exibição do Resultado ---\n",
    "    print(\"DataFrames unidos com sucesso!\")\n",
    "    print(\"Visualizando as 5 primeiras linhas do DataFrame resultante:\")\n",
    "    # O display é mais adequado para notebooks, pois formata a tabela de forma mais legível.\n",
    "    display(df_completo.head())\n",
    "\n",
    "    print(\"\\nInformações sobre o DataFrame completo:\")\n",
    "    df_completo.info()\n",
    "\n",
    "    # Exibe a quantidade de valores nulos para verificar prontuários que não encontraram correspondência\n",
    "    print(\"\\nContagem de valores nulos na coluna 'Prontuário' (do df_pacientes_limpo):\")\n",
    "    print(f\"Isto indica quantos prontuários de 'df_consolidado' não foram encontrados em 'df_pacientes_limpo'.\")\n",
    "    print(df_completo['Prontuário'].isnull().sum())\n",
    "\n",
    "else:\n",
    "    print(\"Erro: Certifique-se de que as células anteriores que criam os DataFrames 'df_consolidado' e 'df_pacientes_limpo' foram executadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Verificação do DataFrame ---\n",
    "# Garante que o DataFrame 'df_completo' existe no ambiente do notebook.\n",
    "if 'df_completo' in locals():\n",
    "\n",
    "    print(f\"Número de linhas ANTES da limpeza: {len(df_completo)}\")\n",
    "\n",
    "    # --- 2. Limpeza dos Dados ---\n",
    "    # A maneira mais robusta de remover linhas com valores ausentes é usando o método .dropna().\n",
    "    # Este método remove, por padrão, linhas que contêm valores nulos (NaN - Not a Number).\n",
    "    # O parâmetro 'subset' especifica que devemos olhar apenas para a coluna 'prontuario_cisam'\n",
    "    # para decidir se uma linha deve ser removida.\n",
    "    df_completo_limpo = df_completo.dropna(subset=['prontuario_cisam'])\n",
    "\n",
    "    # --- Verificação Adicional ---\n",
    "    # Às vezes, \"vazio\" pode significar uma string vazia ('') em vez de um valor nulo (NaN).\n",
    "    # O código abaixo garante que linhas com strings vazias (ou apenas com espaços) também sejam removidas.\n",
    "    # Primeiro, garantimos que a coluna é do tipo string para usar métodos de string (.str)\n",
    "    df_completo_limpo['prontuario_cisam'] = df_completo_limpo['prontuario_cisam'].astype(str)\n",
    "    # Em seguida, mantemos apenas as linhas onde a coluna, sem espaços, não é uma string vazia.\n",
    "    df_completo_limpo = df_completo_limpo[df_completo_limpo['prontuario_cisam'].str.strip() != '']\n",
    "\n",
    "\n",
    "    # --- 3. Exibição do Resultado ---\n",
    "    print(f\"Número de linhas APÓS a limpeza: {len(df_completo_limpo)}\")\n",
    "\n",
    "    print(\"\\nVisualizando as primeiras linhas do DataFrame após a remoção:\")\n",
    "    display(df_completo_limpo.head())\n",
    "\n",
    "    # Opcional: Se você quiser que a variável original 'df_completo' aponte para\n",
    "    # o dataframe limpo, descomente a linha abaixo.\n",
    "    # df_completo = df_completo_limpo.copy()\n",
    "    # print(\"\\nO DataFrame 'df_completo' foi atualizado.\")\n",
    "\n",
    "else:\n",
    "    print(\"Erro: O DataFrame 'df_completo' não foi encontrado. Certifique-se de que a célula anterior (que une os dataframes) foi executada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da80170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que o DataFrame 'df_completo_limpo' já esteja em memória.\n",
    "\n",
    "# 1. Obter a contagem de cada valor único\n",
    "contagem_condicao_saude = df_completo_limpo['condicao_saude'].value_counts()\n",
    "\n",
    "# 2. Converter a Series resultante em um DataFrame para melhor manipulação\n",
    "df_contagem = contagem_condicao_saude.to_frame()\n",
    "\n",
    "# 3. Transformar o índice (que contém os nomes das condições) em uma coluna\n",
    "df_contagem = df_contagem.reset_index()\n",
    "\n",
    "# 4. Renomear as colunas para nomes mais claros e descritivos\n",
    "df_contagem.columns = ['Condicao', 'Frequencia']\n",
    "\n",
    "# 5. Definir o caminho e o nome do arquivo de saída\n",
    "caminho_saida = '../data/interim/'\n",
    "nome_arquivo = 'analise_frequencia_condicao_saude.csv'\n",
    "caminho_completo = os.path.join(caminho_saida, nome_arquivo)\n",
    "\n",
    "# 6. Garantir que o diretório de destino exista\n",
    "os.makedirs(caminho_saida, exist_ok=True)\n",
    "\n",
    "# 7. Salvar o DataFrame em um arquivo CSV\n",
    "# O argumento index=False evita que o índice do DataFrame (0, 1, 2...) seja salvo no arquivo\n",
    "df_contagem.to_csv(caminho_completo, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Análise de frequência da coluna 'condicao_saude' concluída.\")\n",
    "print(f\"Os resultados foram salvos com sucesso em: '{caminho_completo}'\")\n",
    "\n",
    "# Opcional: Exibir as primeiras linhas do DataFrame gerado\n",
    "print(\"\\nAmostra dos dados salvos:\")\n",
    "print(df_contagem.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8118949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que o DataFrame 'df_completo_limpo' já esteja em memória e pronto.\n",
    "\n",
    "# 1. Definir o caminho de saída e o nome do arquivo\n",
    "caminho_saida = '../data/processed/'\n",
    "nome_arquivo = 'dados_pacientes_histeroscopia_limpo.csv'\n",
    "caminho_completo = os.path.join(caminho_saida, nome_arquivo)\n",
    "\n",
    "# 2. Garantir que o diretório de destino exista\n",
    "os.makedirs(caminho_saida, exist_ok=True)\n",
    "\n",
    "# 3. Salvar o DataFrame em um arquivo CSV\n",
    "#   - index=False: Evita que o índice numérico do pandas seja salvo como uma coluna no CSV.\n",
    "#   - encoding='utf-8': Garante a compatibilidade e a correta gravação de caracteres especiais (acentos, etc.).\n",
    "df_completo_limpo.to_csv(caminho_completo, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"DataFrame 'df_completo_limpo' salvo com sucesso!\")\n",
    "print(f\"O arquivo está localizado em: '{caminho_completo}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL do endpoint da API do Ollama para geração de texto\n",
    "# url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# # Dados da requisição\n",
    "# data = {\n",
    "#     \"model\": \"deepseek-r1:14b\",\n",
    "#     \"prompt\": \"Por que o céu é azul?\",\n",
    "#     \"stream\": False  # Definir como False para receber a resposta completa de uma vez\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Fazendo a requisição POST para a API\n",
    "#     response = requests.post(url, json=data)\n",
    "    \n",
    "#     # Verificando se a requisição foi bem-sucedida\n",
    "#     response.raise_for_status()\n",
    "\n",
    "#     # Convertendo a resposta de texto JSON para um dicionário Python\n",
    "#     response_data = response.json()\n",
    "\n",
    "#     # Imprimindo a resposta do modelo\n",
    "#     print(\"\\nResposta do deepseek-r1:14b:\")\n",
    "#     print(response_data['response'])\n",
    "\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"Ocorreu um erro ao tentar se conectar com o Ollama: {e}\")\n",
    "#     print(\"Verifique se o Ollama está em execução na porta 11434.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Carrega a planilha para um DataFrame do pandas\n",
    "# try:\n",
    "#     df = pd.read_csv('../data/interim/criterios_encaminhamento_hd.csv')\n",
    "\n",
    "#     # Cria um dicionário vazio para armazenar os mapeamentos\n",
    "#     criterios_respostas = {}\n",
    "\n",
    "#     # Itera sobre as linhas do DataFrame\n",
    "#     for index, row in df.iterrows():\n",
    "#         criterio = row['Fator Específico a ser Avaliado']\n",
    "#         opcoes = row['Opções Categóricas para o Modelo de IA']\n",
    "        \n",
    "#         # Verifica se os valores não são nulos\n",
    "#         if pd.notna(criterio) and pd.notna(opcoes):\n",
    "#             # Divide as opções de resposta e remove espaços em branco\n",
    "#             respostas = [resposta.strip() for resposta in opcoes.split('/')]\n",
    "            \n",
    "#             # Adiciona a opção \"Não informado\" a lista de respostas\n",
    "#             respostas.append('Não informado')\n",
    "            \n",
    "#             criterios_respostas[criterio] = respostas\n",
    "\n",
    "#     # Adiciona o novo critério manualmente ao final do dicionário\n",
    "#     criterios_respostas['Paciente encaminha para hd'] = ['sim', 'não', 'Não informado']\n",
    "\n",
    "#     # Imprime o dicionário resultante em formato JSON\n",
    "#     import json\n",
    "#     print(json.dumps(criterios_respostas, indent=4, ensure_ascii=False))\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Erro: O arquivo 'criterios_encaminhamento_hd.csv' não foi encontrado.\")\n",
    "#     print(\"Por favor, certifique-se de que o arquivo está no mesmo diretório que o seu notebook Jupyter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73610ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Carrega o arquivo CSV em um DataFrame\n",
    "    df = pd.read_csv('../data/processed/dados_pacientes_histeroscopia_limpo.csv')\n",
    "\n",
    "    # Define as colunas que serão concatenadas\n",
    "    colunas_texto = ['condicao_saude', 'motivo_consulta', 'Evolução']\n",
    "    \n",
    "    # Preenche os valores ausentes (NaN) com uma string vazia ''\n",
    "    for col in colunas_texto:\n",
    "        df[col] = df[col].fillna('')\n",
    "\n",
    "    # Concatena as três colunas em uma nova coluna chamada 'texto_completo'\n",
    "    df['texto_completo'] = df['condicao_saude'] + ' | ' + df['motivo_consulta'] + ' | ' + df['Evolução']\n",
    "\n",
    "    # Remove as colunas de texto originais\n",
    "    df = df.drop(columns=colunas_texto)\n",
    "    \n",
    "    # Define o nome do novo arquivo\n",
    "    novo_arquivo_csv = '../data/processed/dados_pacientes_concatenado.csv'\n",
    "\n",
    "    # Salva o DataFrame modificado em um novo arquivo CSV\n",
    "    # O argumento index=False evita que o pandas salve o índice do DataFrame como uma coluna\n",
    "    df.to_csv(novo_arquivo_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"Arquivo '{novo_arquivo_csv}' gerado com sucesso!\")\n",
    "    print(\"\\nVisualização das primeiras linhas do novo arquivo:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo 'dados_pacientes.csv' não foi encontrado.\")\n",
    "    print(\"Por favor, certifique-se de que o arquivo está no mesmo diretório que o seu notebook Jupyter.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Erro: Uma das colunas necessárias não foi encontrada no arquivo. Verifique o nome da coluna: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # ETAPA 2: CARREGAR OS DADOS DOS PACIENTES\n",
    "# # ==============================================================================\n",
    "# try:\n",
    "#     df_pacientes = pd.read_csv('../data/processed/dados_pacientes_concatenado.csv')\n",
    "#     print(f\"\\nArquivo 'dados_pacientes_concatenado.csv' carregado. Total de {len(df_pacientes)} pacientes para processar.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"ERRO CRÍTICO: O arquivo 'dados_pacientes_concatenado.csv' não foi encontrado.\")\n",
    "#     df_pacientes = None\n",
    "# except Exception as e:\n",
    "#     print(f\"ERRO ao carregar os dados dos pacientes: {e}\")\n",
    "#     df_pacientes = None\n",
    "\n",
    "# # ==============================================================================\n",
    "# # ETAPA 3: FUNÇÃO PARA INTERAGIR COM O DEEPSEEK (DEEPSEEK-R1)\n",
    "# # ==============================================================================\n",
    "\n",
    "# def extrair_informacoes_com_deepseek(texto_paciente, criterios):\n",
    "#     \"\"\"\n",
    "#     Envia o texto do paciente e os critérios para o Ollama e retorna as respostas.\n",
    "#     \"\"\"\n",
    "#     # URL da API do Ollama que roda localmente\n",
    "#     url_ollama = 'http://localhost:11434/api/generate'\n",
    "\n",
    "#     # Construindo o \"prompt\" (a instrução) para o modelo de linguagem\n",
    "#     # Esta é a parte mais importante, pois ensina o modelo como se comportar.\n",
    "#     prompt_completo = f\"\"\"\n",
    "#     Você é um assistente de análise de dados de saúde.\n",
    "#     Sua tarefa é analisar o prontuário de um paciente e responder a uma lista de perguntas.\n",
    "#     Para cada pergunta, você DEVE escolher UMA das opções fornecidas.\n",
    "#     Baseie-se ESTRITAMENTE no texto fornecido. Se a informação não estiver no texto, escolha 'Não informado'.\n",
    "\n",
    "#     **Texto do Prontuário do Paciente:**\n",
    "#     \"{texto_paciente}\"\n",
    "\n",
    "#     **Perguntas e Opções (responda em formato JSON):**\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Adiciona cada critério ao prompt\n",
    "#     perguntas_json = {}\n",
    "#     for criterio, opcoes in criterios.items():\n",
    "#         perguntas_json[criterio] = f\"Escolha uma: {opcoes}\"\n",
    "    \n",
    "#     prompt_completo += json.dumps(perguntas_json, indent=2, ensure_ascii=False)\n",
    "    \n",
    "#     prompt_completo += \"\\n\\n**Responda APENAS com o objeto JSON contendo as chaves (critérios) e os valores (respostas escolhidas).**\"\n",
    "\n",
    "#     # Dados a serem enviados para a API do Ollama\n",
    "#     data = {\n",
    "#         \"model\": \"deepseek-r1:14b\",  # Modelo especificado\n",
    "#         \"prompt\": prompt_completo,\n",
    "#         \"stream\": False,  # Queremos a resposta completa de uma vez\n",
    "#         \"options\": {\n",
    "#             \"temperature\": 0.0 # Baixa temperatura para respostas mais diretas e menos criativas\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(url_ollama, json=data)\n",
    "#         response.raise_for_status()  # Lança um erro se a requisição falhar\n",
    "        \n",
    "#         # O Ollama retorna um JSON, e a resposta do modelo está na chave \"response\"\n",
    "#         resposta_texto = response.json().get('response', '{}')\n",
    "        \n",
    "#         # Tenta extrair o JSON da resposta do modelo\n",
    "#         try:\n",
    "#             # O modelo pode retornar o JSON dentro de ```json ... ```, vamos limpar isso.\n",
    "#             if '```json' in resposta_texto:\n",
    "#                 resposta_texto = resposta_texto.split('```json\\n')[1].split('```')[0]\n",
    "            \n",
    "#             return json.loads(resposta_texto)\n",
    "#         except (json.JSONDecodeError, IndexError) as e:\n",
    "#             print(f\"\\n   -> Aviso: Não foi possível decodificar o JSON da resposta do modelo. Erro: {e}\")\n",
    "#             print(f\"   -> Resposta recebida: {resposta_texto}\")\n",
    "#             return None # Retorna None se o JSON for inválido\n",
    "\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"\\nERRO DE CONEXÃO: Não foi possível conectar ao Ollama. Verifique se ele está rodando. Detalhes: {e}\")\n",
    "#         return \"ERRO_CONEXAO\"\n",
    "\n",
    "# # ==============================================================================\n",
    "# # ETAPA 4: ITERAR, PROCESSAR E SALVAR OS RESULTADOS\n",
    "# # ==============================================================================\n",
    "# if criterios_respostas is not None and df_pacientes is not None:\n",
    "    \n",
    "#     # Cria novas colunas no DataFrame para cada critério, inicializadas com 'Pendente'\n",
    "#     for criterio in criterios_respostas.keys():\n",
    "#         df_pacientes[criterio] = 'Pendente'\n",
    "\n",
    "#     print(\"\\nIniciando o processamento dos pacientes com o modelo deepseek-r1...\")\n",
    "#     print(\"-\" * 70)\n",
    "\n",
    "#     # Itera por cada linha (paciente) do DataFrame\n",
    "#     for index, row in df_pacientes.iterrows():\n",
    "#         texto = row['texto_completo']\n",
    "        \n",
    "#         print(f\"Processando paciente {index + 1} de {len(df_pacientes)}...\")\n",
    "        \n",
    "#         # Chama a função que interage com o Ollama\n",
    "#         respostas_modelo = extrair_informacoes_com_deepseek(texto, criterios_respostas)\n",
    "        \n",
    "#         # Se a conexão com o Ollama falhar, interrompe o script\n",
    "#         if respostas_modelo == \"ERRO_CONEXAO\":\n",
    "#             print(\"Execução interrompida devido a erro de conexão.\")\n",
    "#             break\n",
    "        \n",
    "#         # Se o modelo retornou respostas válidas, atualiza o DataFrame\n",
    "#         if respostas_modelo:\n",
    "#             for criterio, resposta in respostas_modelo.items():\n",
    "#                 if criterio in df_pacientes.columns:\n",
    "#                     # df.at é uma forma eficiente de atribuir um valor a uma célula específica\n",
    "#                     df_pacientes.at[index, criterio] = resposta\n",
    "#         else:\n",
    "#             print(f\"   -> Falha ao obter respostas para o paciente {index + 1}.\")\n",
    "\n",
    "#         # Pausa para não sobrecarregar o servidor e para podermos ler o log\n",
    "#         time.sleep(1) \n",
    "\n",
    "#     print(\"-\" * 70)\n",
    "#     print(\"Processamento concluído!\")\n",
    "\n",
    "#     # Salva o resultado final em um novo arquivo CSV\n",
    "#     arquivo_final = 'dados_pacientes_analisados.csv'\n",
    "#     df_pacientes.to_csv(arquivo_final, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "#     print(f\"\\nArquivo '{arquivo_final}' salvo com sucesso!\")\n",
    "#     print(\"\\nVisualização das 5 primeiras linhas do resultado final:\")\n",
    "#     print(df_pacientes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# # A lista 'messages' irá armazenar o histórico da conversa\n",
    "# messages = [\n",
    "#     {\n",
    "#         'role': 'user',\n",
    "#         'content': 'Olá! Eu quero que você atue como um tradutor de inglês para português do Brasil.',\n",
    "#     },\n",
    "#     {\n",
    "#         'role': 'assistant',\n",
    "#         'content': 'Entendido! Pode me enviar as frases em inglês que eu as traduzirei para o português brasileiro.',\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# def conversar(prompt_usuario):\n",
    "#     \"\"\"Adiciona a nova mensagem do usuário e obtém a resposta do modelo.\"\"\"\n",
    "    \n",
    "#     # Adiciona a nova mensagem ao histórico\n",
    "#     messages.append({\n",
    "#         'role': 'user',\n",
    "#         'content': prompt_usuario,\n",
    "#     })\n",
    "\n",
    "#     # Chama a API de chat, enviando todo o histórico\n",
    "#     response = ollama.chat(\n",
    "#         model='phi3:mini',\n",
    "#         messages=messages\n",
    "#     )\n",
    "\n",
    "#     # Adiciona a resposta do modelo ao histórico\n",
    "#     resposta_assistente = response['message']['content']\n",
    "#     messages.append({\n",
    "#         'role': 'assistant',\n",
    "#         'content': resposta_assistente,\n",
    "#     })\n",
    "    \n",
    "#     return resposta_assistente\n",
    "\n",
    "# # Iniciando a conversa\n",
    "# print(\"Assistente: Entendido! Pode me enviar as frases em inglês que eu as traduzirei para o português brasileiro.\\n\")\n",
    "\n",
    "# # Primeira interação\n",
    "# prompt1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# print(f\"Você: {prompt1}\")\n",
    "# resposta1 = conversar(prompt1)\n",
    "# print(f\"Assistente: {resposta1}\\n\")\n",
    "\n",
    "# # Segunda interação (o modelo se lembrará do contexto)\n",
    "# prompt2 = \"How about this one: 'Never gonna give you up, never gonna let you down.'\"\n",
    "# print(f\"Você: {prompt2}\")\n",
    "# resposta2 = conversar(prompt2)\n",
    "# print(f\"Assistente: {resposta2}\\n\")\n",
    "\n",
    "# # Você pode inspecionar o histórico completo se quiser\n",
    "# # print(\"\\n--- Histórico da Conversa ---\")\n",
    "# # print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73869d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados a serem armazenados no arquivo JSON\n",
    "# dados_json = {\n",
    "#     \"Padrão do Sangramento Uterino\": [\n",
    "#         \"Ciclos Regulares\",\n",
    "#         \"Ciclos Irregulares\",\n",
    "#         \"Ausência de Menstruação (Amenorreia)\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Volume do Fluxo Menstrual\": [\n",
    "#         \"Normal\",\n",
    "#         \"Aumentado (troca de absorvente a cada 1-2h)\",\n",
    "#         \"Diminuído\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Sangramento Pós-Menopausa\": [\n",
    "#         \"Sim\",\n",
    "#         \"Não\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Sangramento Fora do Período\": [\n",
    "#         \"Presente\",\n",
    "#         \"Ausente\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Dificuldade para Engravidar (Infertilidade)\": [\n",
    "#         \"Sim (tentando há >1 ano)\",\n",
    "#         \"Sim (tentando há <1 ano)\",\n",
    "#         \"Não se aplica\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Histórico de Abortos de Repetição\": [\n",
    "#         \"Nenhum\",\n",
    "#         \"1 aborto\",\n",
    "#         \"2 ou mais abortos\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Dor Pélvica Crônica ou Cólicas Severas\": [\n",
    "#         \"Presente\",\n",
    "#         \"Ausente\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Uso de Tamoxifeno\": [\n",
    "#         \"Sim\",\n",
    "#         \"Não\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Terapia de Reposição Hormonal (TRH)\": [\n",
    "#         \"Sim\",\n",
    "#         \"Não\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Histórico Familiar Relevante\": [\n",
    "#         \"Presente\",\n",
    "#         \"Ausente\",\n",
    "#         \"Desconhecido\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Sangramento Ativo no Exame Especular\": [\n",
    "#         \"Presente\",\n",
    "#         \"Ausente\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Aumento do Volume Uterino na Palpação\": [\n",
    "#         \"Sim (aumentado)\",\n",
    "#         \"Não (tamanho normal)\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Dificuldade em Localizar o Fio do DIU\": [\n",
    "#         \"Sim (fio não visível)\",\n",
    "#         \"Não se aplica\",\n",
    "#         \"Não informado\"\n",
    "#     ],\n",
    "#     \"Paciente encaminha para hd\": [\n",
    "#         \"sim\",\n",
    "#         \"não\",\n",
    "#         \"Não informado\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Define o caminho e o nome do arquivo\n",
    "# caminho_arquivo = \"../data/interim/criterios_avaliacao.json\"\n",
    "\n",
    "# Extrai o diretório do caminho do arquivo\n",
    "# diretorio = os.path.dirname(caminho_arquivo)\n",
    "\n",
    "# Cria o diretório se ele não existir\n",
    "# if not os.path.exists(diretorio):\n",
    "#     os.makedirs(diretorio)\n",
    "\n",
    "# Escreve o dicionário no arquivo JSON com formatação e codificação UTF-8\n",
    "# with open(caminho_arquivo, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(dados_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Imprime uma mensagem de confirmação\n",
    "# print(f\"Arquivo salvo com sucesso em: {caminho_arquivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Leitura do JSON\n",
    "caminho_json = '../data/interim/criterios_respostas.json'\n",
    "with open(caminho_json, 'r', encoding='utf-8') as f:\n",
    "    criterios_objeto = json.load(f)\n",
    "\n",
    "# 3. Definir os caminhos dos arquivos de entrada e saída\n",
    "caminho_entrada = '../data/processed/dados_pacientes_concatenado.csv'\n",
    "caminho_saida = '../data/processed/resultado_final_v2.csv'\n",
    "\n",
    "# 4. Bloco principal de execução com tratamento de erros\n",
    "try:\n",
    "    # Ler o arquivo CSV para um DataFrame do pandas\n",
    "    print(f\"Lendo o arquivo de entrada: {caminho_entrada}\")\n",
    "    df = pd.read_csv(caminho_entrada)\n",
    "    print(\"Arquivo lido com sucesso!\")\n",
    "    print(f\"Shape original do DataFrame: {df.shape}\")\n",
    "    print(\"\\nPrimeiras 5 linhas do DataFrame original:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Obter a lista de novas colunas a partir das chaves do JSON\n",
    "    novas_colunas = list(criterio['nome_coluna'] for criterio in criterios_objeto)\n",
    "    print(f\"\\nAdicionando {len(novas_colunas)} novas colunas vazias...\")\n",
    "\n",
    "    # Adicionar cada chave do JSON como uma nova coluna no DataFrame\n",
    "    # O valor atribuído será NaN (Not a Number), que é o padrão do pandas para dados ausentes.\n",
    "    for coluna in novas_colunas:\n",
    "        df[coluna] = \"\"\n",
    "\n",
    "    print(\"Novas colunas adicionadas com sucesso!\")\n",
    "    print(f\"Novo shape do DataFrame: {df.shape}\")\n",
    "    print(\"\\nPrimeiras 5 linhas do DataFrame com as novas colunas:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Salvar o DataFrame modificado em um novo arquivo CSV\n",
    "    # O argumento index=False evita que o pandas salve o índice do DataFrame como uma coluna no CSV\n",
    "    print(f\"\\nSalvando o resultado em: {caminho_saida}\")\n",
    "    df.to_csv(caminho_saida, index=False)\n",
    "    print(\"Arquivo 'resultado_final.csv' salvo com sucesso!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O arquivo de entrada não foi encontrado no caminho especificado: {caminho_entrada}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado durante o processamento: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os caminhos para os arquivos de dados.\n",
    "caminho_csv = '../data/processed/resultado_final_v2.csv'\n",
    "caminho_resultado_final_processado = '../data/processed/resultado_final_processado_v2.csv'\n",
    "caminho_json = '../data/interim/criterios_respostas.json'\n",
    "\n",
    "colunas_para_tratar_como_string = {\n",
    "    'padrao_sangramento_uterino': 'str',\n",
    "    'volume_fluxo_menstrual': 'str',\n",
    "    'sangramento_pos_menopausa': 'str',\n",
    "    'sangramento_fora_periodo': 'str',\n",
    "    'dificuldade_engravidar': 'str',\n",
    "    'historico_aborto': 'str',\n",
    "    'dor_pelvica': 'str',\n",
    "    'tamoxifeno': 'str',\n",
    "    'terapia_reposição_hormonal_trh': 'str',\n",
    "    'historico_familiar': 'str',\n",
    "    'sangramento_ativo_exame_specular': 'str',\n",
    "    'aumento_volume_uterino_palpacao': 'str',\n",
    "    'localizar_fio_diu': 'str',\n",
    "    'encaminhada_hd': 'str'\n",
    "}\n",
    "\n",
    "print('-' * 50)\n",
    "print('OP ID: 001')\n",
    "print('Início da execução do algoritmo')\n",
    "print('-' * 50)\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# Carrega o arquivo JSON. \n",
    "# A estrutura esperada é um objeto onde cada chave tem uma lista de strings como valor.\n",
    "# Ex: {\"chave1\": [\"opcaoA\", \"opcaoB\"], \"chave2\": [\"opcaoC\", \"opcaoD\"]}\n",
    "with open(caminho_json, 'r', encoding='utf-8') as f:\n",
    "    criterios_objeto = json.load(f)\n",
    "\n",
    "# Carrega o DataFrame a partir do CSV.\n",
    "df_pacientes = pd.read_csv(caminho_csv, dtype=colunas_para_tratar_como_string)\n",
    "\n",
    "# Armazena a quantidade de voltas do loop no dataframe. Tem utilizade para a depuração do código\n",
    "index_loop = 0\n",
    "\n",
    "# Itera sobre cada paciente no DataFrame.\n",
    "for linha_paciente in df_pacientes.itertuples():\n",
    "    index_loop += 1\n",
    "    if hasattr(linha_paciente, 'texto_completo'):\n",
    "        \n",
    "        texto_do_paciente = str(linha_paciente.texto_completo)\n",
    "\n",
    "        print('-' * 50)\n",
    "        print('OP ID: 002')\n",
    "        print(f'Iniciando execução para paciente {index_loop}')\n",
    "        print('-' * 50)\n",
    "        print('\\n\\n')\n",
    "\n",
    "        # Itera sobre as chaves do objeto JSON (os critérios).\n",
    "        for criterio in criterios_objeto:\n",
    "            \n",
    "            # --- ALTERAÇÃO: Acessa a lista de opções de resposta para o critério atual. ---\n",
    "            opcoes_de_resposta = criterio['alternativas']\n",
    "            \n",
    "            # Formata a lista de opções em uma string clara para o prompt.\n",
    "            # Ex: \"'Opção 1', 'Opção 2', 'Opção 3'\"\n",
    "            opcoes_formatadas = \", \".join([f\"{chr(65+i)}) {opcao}\" for i, opcao in enumerate(opcoes_de_resposta)])\n",
    "\n",
    "            # Monta o prompt com a nova instrução.\n",
    "\n",
    "            prompt_atual = (\n",
    "                f\"PRONTUÁRIO DA PACIENTE:\\n{texto_do_paciente}\\n\\n\"\n",
    "                f\"PERGUNTA:\\n {criterio['pergunta']}\\n\"\n",
    "                f\"{opcoes_formatadas}\\n\\n\"\n",
    "                \"COM BASE, SOMENTE, NAS INFORMAÇÕES APRESENTADAS NO PRONTUÁRIO DA PACIENTE, RESPONDA APENAS COM A LETRA QUE RESPONDE A PERGUNTA.\"\n",
    "            )\n",
    "\n",
    "            print('-' * 50)\n",
    "            print('OP ID: 003')\n",
    "            print(f'Mensagem a ser enviada para o modelo: ')\n",
    "            print(prompt_atual)\n",
    "            print('-' * 50)\n",
    "            print('\\n\\n')\n",
    "\n",
    "            response = ollama.generate(\n",
    "                model='llama3:8b',\n",
    "                prompt=prompt_atual\n",
    "            )\n",
    "\n",
    "            if 'response' in response:\n",
    "                resposta_texto = response['response']\n",
    "                tempo_para_resposta = response['total_duration']\n",
    "                \n",
    "                # Salva a resposta do modelo na coluna correspondente do DataFrame.\n",
    "                df_pacientes.loc[linha_paciente.Index, criterio['nome_coluna']] = resposta_texto\n",
    "\n",
    "                print('-' * 50)\n",
    "                print('OP ID: 004')\n",
    "                print(f'Resposta do modelo: {resposta_texto}')\n",
    "                print(f'Tempo para a resposta {tempo_para_resposta / 1e9:.2f}')\n",
    "                print('-' * 50)\n",
    "                print('\\n\\n')\n",
    "            else:\n",
    "                print('-' * 50)\n",
    "                print('OP ID: 004')\n",
    "                print(f'Houve um erro na resposta do modelo para essa iteração {index_loop}')\n",
    "                print('-' * 50)\n",
    "                print('\\n\\n')\n",
    "                df_pacientes.loc[linha_paciente.Index, criterio] = \"ERRO NA RESPOSTA DO MODELO\"\n",
    "    else:\n",
    "        print('-' * 50)\n",
    "        print('OP ID: 002')\n",
    "        print(f'ERRO: atributo \"texto_completo\" não encontrado. Iteração pulada no index {index_loop}')\n",
    "        print('-' * 50)\n",
    "        print('\\n\\n')\n",
    "    print('-' * 50)\n",
    "    print('OP ID: 005')\n",
    "    print(f'Paciente {index_loop} totalmente analisado')\n",
    "    print('-' * 50)\n",
    "    print('\\n\\n')\n",
    "# Salva o DataFrame modificado de volta ao arquivo CSV.\n",
    "df_pacientes.to_csv(caminho_resultado_final_processado, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processamento concluído. O DataFrame foi atualizado e salvo em '{caminho_resultado_final_processado}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenando código útil\n",
    "\n",
    "response = ollama.generate(\n",
    "                model='llama3:8b',\n",
    "                prompt=prompt_atual,\n",
    "                options={\n",
    "                    'num_predict': 1\n",
    "                }\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mineracao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
